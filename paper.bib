@software{vispy,
  author       = {Luke Campagnola and
                  Eric Larson and
                  Almar Klein and
                  David Hoese and
                  Siddharth and
                  Cyrille Rossant and
                  Adam Griffiths and
                  Nicolas P. Rougier and
                  asnt and
                  Kai MÃ¼hlbauer and
                  Alexander Taylor and
                  MSS and
                  Talley Lambert and
                  sylm21 and
                  Alex J. Champandard and
                  Max Hunter and
                  Thomas Robitaille and
                  Mustafa Furkan Kaptan and
                  Elliott Sales de Andrade and
                  Karl Czajkowski and
                  Lorenzo Gaifas and
                  Alessandro Bacchini and
                  Guillaume Favelier and
                  Etienne Combrisson and
                  ThenTech and
                  fschill and
                  Mark Harfouche and
                  Michael Aye and
                  Casper van Elteren and
                  Cedric GESTES},
  title        = {vispy/vispy: Version 0.11.0},
  month        = jul,
  year         = 2022,
  publisher    = {Zenodo},
  version      = {v0.11.0},
  doi          = {10.5281/zenodo.6795163},
  url          = {https://doi.org/10.5281/zenodo.6795163}
}



@article{bento,
  abstract = {The study of naturalistic social behavior requires quantification of animals' interactions. This is generally done through manual annotation---a highly time-consuming and tedious process. Recent advances in computer vision enable tracking the pose (posture) of freely behaving animals. However, automatically and accurately classifying complex social behaviors remains technically challenging. We introduce the Mouse Action Recognition System (MARS), an automated pipeline for pose estimation and behavior quantification in pairs of freely interacting mice. We compare MARS's annotations to human annotations and find that MARS's pose estimation and behavior classification achieve human-level performance. We also release the pose and annotation datasets used to train MARS to serve as community benchmarks and resources. Finally, we introduce the Behavior Ensemble and Neural Trajectory Observatory (BENTO), a graphical user interface for analysis of multimodal neuroscience datasets. Together, MARS and BENTO provide an end-to-end pipeline for behavior data extraction and analysis in a package that is user-friendly and easily modifiable.},
  article_type = {journal},
  author = {Segalin, Cristina and Williams, Jalani and Karigo, Tomomi and Hui, May and Zelikowsky, Moriel and Sun, Jennifer J and Perona, Pietro and Anderson, David J and Kennedy, Ann},
  citation = {eLife 2021;10:e63720},
  date-modified = {2022-10-05 13:18:49 -0400},
  doi = {10.7554/eLife.63720},
  editor = {Berman, Gordon J and Wassum, Kate M and Gal, Asaf},
  issn = {2050-084X},
  journal = {eLife},
  keywords = {social behavior, pose estimation, machine learning, computer vision, microendoscopic imaging, software},
  month = {nov},
  pages = {e63720},
  pub_date = {2021-11-30},
  publisher = {eLife Sciences Publications, Ltd},
  title = {The Mouse Action Recognition System (MARS) software pipeline for automated analysis of social behaviors in mice},
  url = {https://doi.org/10.7554/eLife.63720},
  volume = 10,
  year = 2021,
  bdsk-url-1 = {https://doi.org/10.7554/eLife.63720}}


@misc{rastermap,
  author = {C. Stringer and M. Pachitariu},
  title = {rastermap},
  year = {2020},
  publisher = {GitHub},
  journal = {GitHub repository},
  url = {https://github.com/MouseLand/rastermap}
}

@misc{vidio,
  author = {J. Bohnslav},
  title = {VidIO: simple, performant video reading and writing in python},
  year = {2020},
  publisher = {GitHub},
  journal = {GitHub repository},
  url = {https://github.com/jbohnslav/vidio}
}

